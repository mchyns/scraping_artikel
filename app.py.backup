from flask import Flask, render_template, request, jsonify, send_file, make_response
import requests
from bs4 import BeautifulSoup
import json
import os
import csv
import io
from datetime import datetime, timedelta
from urllib.parse import urlencode
import time
import re

app = Flask(__name__)

# Daftar topik BPS berdasarkan klasifikasi yang diberikan
BPS_TOPICS = {
    "A": {
        "name": "Pertanian, Kehutanan, dan Perikanan",
        "subcategories": {
            "1": {
                "name": "Pertanian, Peternakan, Perburuan dan Jasa Pertanian",
                "subtopics": [
                    "Tanaman Pangan",
                    "Tanaman Hortikultura Semusim", 
                    "Perkebunan Semusim",
                    "Tanaman Hortikultura Tahunan",
                    "Perkebunan Tahunan",
                    "Peternakan",
                    "Jasa Pertanian dan Perburuan"
                ]
            },
            "2": {"name": "Kehutanan dan Penebangan Kayu"},
            "3": {"name": "Perikanan"}
        }
    },
    "B": {
        "name": "Pertambangan dan Penggalian",
        "subcategories": {
            "1": {"name": "Pertambangan Minyak, Gas dan Panas Bumi"},
            "2": {"name": "Pertambangan Batubara dan Lignit"},
            "3": {"name": "Pertambangan Bijih Logam"},
            "4": {"name": "Pertambangan dan Penggalian Lainnya"}
        }
    },
    "C": {
        "name": "Industri Pengolahan",
        "subcategories": {
            "1": {
                "name": "Industri Batubara dan Pengilangan Migas",
                "subtopics": ["Industri Batu Bara", "Pengilangan Migas"]
            },
            "2": {"name": "Industri Makanan dan Minuman"},
            "3": {"name": "Pengolahan Tembakau"},
            "4": {"name": "Industri Tekstil dan Pakaian Jadi"},
            "5": {"name": "Industri Kulit, Barang dari Kulit dan Alas Kaki"},
            "6": {"name": "Industri Kayu, Barang dari Kayu dan Gabus"},
            "7": {"name": "Industri Kertas dan Barang dari Kertas"},
            "8": {"name": "Industri Kimia, Farmasi dan Obat Tradisional"},
            "9": {"name": "Industri Karet, Barang dari Karet dan Plastik"},
            "10": {"name": "Industri Barang Galian bukan Logam"},
            "11": {"name": "Industri Logam Dasar"},
            "12": {"name": "Industri Barang dari Logam, Komputer, Elektronik"},
            "13": {"name": "Industri Mesin dan Perlengkapan"},
            "14": {"name": "Industri Alat Angkutan"},
            "15": {"name": "Industri Furnitur"},
            "16": {"name": "Industri pengolahan lainnya"}
        }
    },
    "D": {
        "name": "Pengadaan Listrik dan Gas",
        "subcategories": {
            "1": {"name": "Ketenagalistrikan"},
            "2": {"name": "Pengadaan Gas dan Produksi Es"}
        }
    },
    "E": {"name": "Pengadaan Air, Pengelolaan Sampah, Limbah dan Daur Ulang"},
    "F": {"name": "Konstruksi"},
    "G": {
        "name": "Perdagangan Besar dan Eceran",
        "subcategories": {
            "1": {"name": "Perdagangan Mobil, Sepeda Motor dan Reparasinya"},
            "2": {"name": "Perdagangan Besar dan Eceran, Bukan Mobil dan Sepeda Motor"}
        }
    },
    "H": {
        "name": "Transportasi dan Pergudangan",
        "subcategories": {
            "1": {"name": "Angkutan Rel"},
            "2": {"name": "Angkutan Darat"},
            "3": {"name": "Angkutan Laut"},
            "4": {"name": "Angkutan Sungai Danau dan Penyeberangan"},
            "5": {"name": "Angkutan Udara"},
            "6": {"name": "Pergudangan dan Jasa Penunjang Angkutan"}
        }
    },
    "I": {
        "name": "Penyediaan Akomodasi dan Makan Minum",
        "subcategories": {
            "1": {"name": "Penyediaan Akomodasi"},
            "2": {"name": "Penyediaan Makan Minum"}
        }
    },
    "J": {"name": "Informasi dan Komunikasi"},
    "K": {
        "name": "Jasa Keuangan dan Asuransi",
        "subcategories": {
            "1": {"name": "Jasa Perantara Keuangan"},
            "2": {"name": "Asuransi dan Dana Pensiun"},
            "3": {"name": "Jasa Keuangan Lainnya"},
            "4": {"name": "Jasa Penunjang Keuangan"}
        }
    },
    "L": {"name": "Real Estate"},
    "M": {"name": "Jasa Perusahaan"},
    "O": {"name": "Administrasi Pemerintahan, Pertahanan dan Jaminan Sosial"},
    "P": {"name": "Jasa Pendidikan"},
    "Q": {"name": "Jasa Kesehatan dan Kegiatan Sosial"},
    "R": {"name": "Jasa lainnya"}
}

# Daerah target
TARGET_REGIONS = ["Bangkalan", "Madura", "Jawa Timur"]

class GoogleNewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def search_news(self, query, region="", date_range=""):
        """Mencari berita di Google News"""
        search_query = f"{query}"
        if region:
            search_query += f" {region}"
        
        # Parameter untuk Google News
        params = {
            'q': search_query,
            'tbm': 'nws',  # News search
            'hl': 'id',    # Bahasa Indonesia
            'gl': 'id'     # Lokasi Indonesia
        }
        
        if date_range:
            params['tbs'] = f'cdr:1,cd_min:{date_range},cd_max:{date_range}'
            
        try:
            url = f"https://www.google.com/search?{urlencode(params)}"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_results = []
            
            # Mencari elemen berita
            news_items = soup.find_all('div', {'class': 'SoaBEf'}) or soup.find_all('div', {'class': 'xrnccd'})
            
            for item in news_items[:10]:  # Ambil maksimal 10 berita
                try:
                    # Cari link
                    link_elem = item.find('a')
                    if not link_elem:
                        continue
                        
                    link = link_elem.get('href', '')
                    if link.startswith('/url?q='):
                        link = link.split('/url?q=')[1].split('&')[0]
                    elif link.startswith('/search'):
                        continue
                        
                    # Cari judul
                    title_elem = item.find('h3') or item.find('div', {'role': 'heading'})
                    title = title_elem.get_text(strip=True) if title_elem else 'No title'
                    
                    # Cari sumber
                    source_elem = item.find('span', {'class': 'CEMjEf'}) or item.find('cite')
                    source = source_elem.get_text(strip=True) if source_elem else 'Unknown source'
                    
                    # Cari tanggal
                    date_elem = item.find('span', {'class': 'r0bn4c'})
                    date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    if link and title:
                        news_results.append({
                            'title': title,
                            'link': link,
                            'source': source,
                            'date': date,
                            'query': search_query,
                            'scraped_at': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
            return news_results
            
        except Exception as e:
            print(f"Error scraping: {e}")
            return []
    
    def scrape_by_topic(self, topic_code, subcategory=None, region="", date_range=""):
        """Scraping berdasarkan topik BPS"""
        results = []
        
        if topic_code not in BPS_TOPICS:
            return results
            
        topic = BPS_TOPICS[topic_code]
        
        # Jika ada subcategory
        if subcategory and 'subcategories' in topic:
            if subcategory in topic['subcategories']:
                subcat = topic['subcategories'][subcategory]
                query = subcat['name']
                
                # Jika ada subtopics
                if 'subtopics' in subcat:
                    for subtopic in subcat['subtopics']:
                        sub_results = self.search_news(subtopic, region, date_range)
                        for result in sub_results:
                            result['topic_code'] = topic_code
                            result['topic_name'] = topic['name']
                            result['subcategory'] = subtopic
                            result['region'] = region
                        results.extend(sub_results)
                        time.sleep(1)  # Delay untuk menghindari rate limit
                else:
                    sub_results = self.search_news(query, region, date_range)
                    for result in sub_results:
                        result['topic_code'] = topic_code
                        result['topic_name'] = topic['name']
                        result['subcategory'] = query
                        result['region'] = region
                    results.extend(sub_results)
        else:
            # Scraping topik utama
            query = topic['name'] if isinstance(topic, dict) and 'name' in topic else topic
            main_results = self.search_news(query, region, date_range)
            for result in main_results:
                result['topic_code'] = topic_code
                result['topic_name'] = query
                result['region'] = region
            results.extend(main_results)
            
        return results

def save_to_json(data, filename):
    """Menyimpan data ke file JSON"""
    os.makedirs('data', exist_ok=True)
    filepath = f'data/{filename}'
    
    # Load existing data
    existing_data = []
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except:
            existing_data = []
    
    # Menggabungkan data baru dengan data yang ada
    existing_links = {item.get('link', '') for item in existing_data}
    new_data = [item for item in data if item.get('link', '') not in existing_links]
    
    if new_data:
        existing_data.extend(new_data)
        
        # Simpan data
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        return len(new_data)
    return 0

def load_from_json(filename):
    """Memuat data dari file JSON"""
    filepath = f'data/{filename}'
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

@app.route('/')
def index():
    return render_template('index.html', topics=BPS_TOPICS, regions=TARGET_REGIONS)

@app.route('/scrape', methods=['POST'])
def scrape_news():
    try:
        data = request.get_json()
        topic_code = data.get('topic_code', '')
        subcategory = data.get('subcategory', '')
        region = data.get('region', '')
        date_range = data.get('date_range', '')
        
        scraper = GoogleNewsScraper()
        results = scraper.scrape_by_topic(topic_code, subcategory, region, date_range)
        
        if results:
            # Simpan hasil scraping
            filename = f'news_data_{datetime.now().strftime("%Y%m%d")}.json'
            saved_count = save_to_json(results, filename)
            
            return jsonify({
                'status': 'success',
                'message': f'Berhasil scraping {len(results)} berita, {saved_count} data baru disimpan',
                'data': results,
                'total': len(results)
            })
        else:
            return jsonify({
                'status': 'warning',
                'message': 'Tidak ada berita ditemukan',
                'data': [],
                'total': 0
            })
            
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Terjadi kesalahan: {str(e)}',
            'data': [],
            'total': 0
        })

@app.route('/data')
def get_data():
    """Mendapatkan data yang telah disimpan"""
    try:
        topic_filter = request.args.get('topic', '')
        region_filter = request.args.get('region', '')
        date_filter = request.args.get('date', '')
        
        # Load semua data
        all_files = []
        data_dir = 'data'
        if os.path.exists(data_dir):
            for file in os.listdir(data_dir):
                if file.endswith('.json') and file.startswith('news_data_'):
                    all_files.extend(load_from_json(file))
        
        # Filter data
        filtered_data = all_files
        
        if topic_filter:
            filtered_data = [item for item in filtered_data if item.get('topic_code') == topic_filter]
        
        if region_filter:
            filtered_data = [item for item in filtered_data if region_filter.lower() in item.get('region', '').lower()]
        
        if date_filter:
            filtered_data = [item for item in filtered_data if date_filter in item.get('scraped_at', '')]
        
        return jsonify({
            'status': 'success',
            'data': filtered_data,
            'total': len(filtered_data)
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Terjadi kesalahan: {str(e)}',
            'data': [],
            'total': 0
        })

@app.route('/export-csv')
def export_csv():
    """Export data ke file CSV"""
    try:
        # Ambil parameter filter dari query string
        topic_filter = request.args.get('topic', '')
        region_filter = request.args.get('region', '')
        date_filter = request.args.get('date', '')
        
        # Load semua data (sama seperti endpoint /data)
        all_files = []
        data_dir = 'data'
        if os.path.exists(data_dir):
            for file in os.listdir(data_dir):
                if file.endswith('.json') and file.startswith('news_data_'):
                    all_files.extend(load_from_json(file))
        
        # Filter data
        filtered_data = all_files
        
        if topic_filter:
            filtered_data = [item for item in filtered_data if item.get('topic_code') == topic_filter]
        
        if region_filter:
            filtered_data = [item for item in filtered_data if region_filter.lower() in item.get('region', '').lower()]
        
        if date_filter:
            filtered_data = [item for item in filtered_data if date_filter in item.get('scraped_at', '')]
        
        if not filtered_data:
            return jsonify({
                'status': 'error',
                'message': 'Tidak ada data untuk diekspor'
            })
        
        # Buat CSV dalam memory
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Header CSV
        headers = [
            'No',
            'Judul Berita',
            'Link',
            'Sumber',
            'Tanggal Publikasi Berita',
            'Kode Topik BPS',
            'Nama Topik BPS',
            'Sub Kategori',
            'Daerah',
            'Query Pencarian',
            'Tanggal Scraping'
        ]
        writer.writerow(headers)
        
        # Data rows
        for i, item in enumerate(filtered_data, 1):
            # Format waktu scraping
            scraped_time = item.get('scraped_at', '')
            if scraped_time:
                try:
                    dt = datetime.fromisoformat(scraped_time.replace('Z', '+00:00'))
                    scraped_time = dt.strftime('%d/%m/%Y %H:%M:%S')
                except:
                    pass
            
            row = [
                i,
                item.get('title', ''),
                item.get('link', ''),
                item.get('source', ''),
                item.get('date', ''),
                item.get('topic_code', ''),
                item.get('topic_name', ''),
                item.get('subcategory', ''),
                item.get('region', ''),
                item.get('query', ''),
                scraped_time
            ]
            writer.writerow(row)
        
        # Siapkan response
        output.seek(0)
        
        # Nama file berdasarkan filter dan tanggal
        filename_parts = ['bps_news_data']
        if topic_filter:
            filename_parts.append(f'topic_{topic_filter}')
        if region_filter:
            filename_parts.append(f'region_{region_filter}')
        if date_filter:
            filename_parts.append(f'date_{date_filter}')
        
        filename_parts.append(datetime.now().strftime('%Y%m%d_%H%M%S'))
        filename = '_'.join(filename_parts) + '.csv'
        
        # Buat response dengan file CSV
        response = make_response(output.getvalue())
        response.headers['Content-Type'] = 'text/csv; charset=utf-8'
        response.headers['Content-Disposition'] = f'attachment; filename={filename}'
        
        return response
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Gagal mengekspor data: {str(e)}'
        })

@app.route('/data-manager')
def data_manager():
    """Halaman untuk mengelola data yang sudah disimpan"""
    try:
        # Hitung statistik data
        all_files = []
        data_dir = 'data'
        file_stats = []
        
        if os.path.exists(data_dir):
            for file in os.listdir(data_dir):
                if file.endswith('.json') and file.startswith('news_data_'):
                    file_path = os.path.join(data_dir, file)
                    file_data = load_from_json(file)
                    file_size = os.path.getsize(file_path)
                    
                    # Extract tanggal dari nama file
                    date_str = file.replace('news_data_', '').replace('.json', '')
                    try:
                        file_date = datetime.strptime(date_str, '%Y%m%d').strftime('%d %B %Y')
                    except:
                        file_date = date_str
                    
                    file_stats.append({
                        'filename': file,
                        'date': file_date,
                        'count': len(file_data),
                        'size': f"{file_size/1024:.1f} KB"
                    })
                    
                    all_files.extend(file_data)
        
        # Statistik umum
        total_articles = len(all_files)
        
        # Statistik per topik
        topic_stats = {}
        region_stats = {}
        source_stats = {}
        
        for item in all_files:
            # Statistik topik
            topic_code = item.get('topic_code', 'Unknown')
            topic_name = item.get('topic_name', 'Unknown')
            topic_key = f"{topic_code} - {topic_name}"
            topic_stats[topic_key] = topic_stats.get(topic_key, 0) + 1
            
            # Statistik region
            region = item.get('region', 'Unknown')
            region_stats[region] = region_stats.get(region, 0) + 1
            
            # Statistik sumber
            source = item.get('source', 'Unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        # Sort statistik
        topic_stats = dict(sorted(topic_stats.items(), key=lambda x: x[1], reverse=True))
        region_stats = dict(sorted(region_stats.items(), key=lambda x: x[1], reverse=True))
        source_stats = dict(sorted(source_stats.items(), key=lambda x: x[1], reverse=True)[:10])  # Top 10 sources
        
        return render_template('data_manager.html', 
                             topics=BPS_TOPICS,
                             regions=TARGET_REGIONS,
                             file_stats=file_stats,
                             total_articles=total_articles,
                             topic_stats=topic_stats,
                             region_stats=region_stats,
                             source_stats=source_stats)
        
    except Exception as e:
        return render_template('data_manager.html', 
                             topics=BPS_TOPICS,
                             regions=TARGET_REGIONS,
                             error=str(e))

@app.route('/api/delete-file/<filename>')
def delete_file(filename):
    """Menghapus file data"""
    try:
        if not filename.endswith('.json') or not filename.startswith('news_data_'):
            return jsonify({
                'status': 'error',
                'message': 'Nama file tidak valid'
            })
        
        file_path = os.path.join('data', filename)
        if os.path.exists(file_path):
            os.remove(file_path)
            return jsonify({
                'status': 'success',
                'message': f'File {filename} berhasil dihapus'
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'File tidak ditemukan'
            })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Gagal menghapus file: {str(e)}'
        })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)